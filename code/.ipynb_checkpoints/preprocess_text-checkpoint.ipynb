{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minería de texto: clasificación de artículos\n",
    "\n",
    "### Autor: Silvia García Hernández                                                                    \n",
    "### Fecha: 3/06/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pybtex.database.input import bibtex\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import string\n",
    "import pickle \n",
    "import time\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading the dataset\n",
    "\n",
    "The dataset has 3 columns of interest (title, keywords and abstract). A fouth column will be created to compare the classification scores of the four cases. The idea is to preprocess the columns, vectorized them and then implement a set of machine learning classifications models to compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AZAZA20181</td>\n",
       "      <td>Context proposals for saliency detection</td>\n",
       "      <td>Computational saliency, Object segmentation, O...</td>\n",
       "      <td>One of the fundamental properties of a salient...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Computer Vision and Image Understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BENSHABAT201812</td>\n",
       "      <td>Graph based over-segmentation methods for 3D p...</td>\n",
       "      <td>3D point cloud over-segmentation, 3D point clo...</td>\n",
       "      <td>Over-segmentation, or super-pixel generation, ...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Computer Vision and Image Understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YANG201843</td>\n",
       "      <td>Text effects transfer via distribution-aware t...</td>\n",
       "      <td>Text effects, Texture synthesis, Spatial distr...</td>\n",
       "      <td>In this paper, we explore the problem of fanta...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Computer Vision and Image Understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BARATH201870</td>\n",
       "      <td>Efficient energy-based topological outlier rej...</td>\n",
       "      <td>Stereo vision, Outlier filtering, Energy minim...</td>\n",
       "      <td>An approach is proposed for outlier rejection ...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Computer Vision and Image Understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARRIGONI201895</td>\n",
       "      <td>Robust synchronization in SO(3) and SE(3) via ...</td>\n",
       "      <td>Absolute rotations, Global rotations, Structur...</td>\n",
       "      <td>This paper deals with the synchronization prob...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Computer Vision and Image Understanding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             entry                                              title  \\\n",
       "0       AZAZA20181           Context proposals for saliency detection   \n",
       "1  BENSHABAT201812  Graph based over-segmentation methods for 3D p...   \n",
       "2       YANG201843  Text effects transfer via distribution-aware t...   \n",
       "3     BARATH201870  Efficient energy-based topological outlier rej...   \n",
       "4   ARRIGONI201895  Robust synchronization in SO(3) and SE(3) via ...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  Computational saliency, Object segmentation, O...   \n",
       "1  3D point cloud over-segmentation, 3D point clo...   \n",
       "2  Text effects, Texture synthesis, Spatial distr...   \n",
       "3  Stereo vision, Outlier filtering, Energy minim...   \n",
       "4  Absolute rotations, Global rotations, Structur...   \n",
       "\n",
       "                                            abstract  year  \\\n",
       "0  One of the fundamental properties of a salient...  2018   \n",
       "1  Over-segmentation, or super-pixel generation, ...  2018   \n",
       "2  In this paper, we explore the problem of fanta...  2018   \n",
       "3  An approach is proposed for outlier rejection ...  2018   \n",
       "4  This paper deals with the synchronization prob...  2018   \n",
       "\n",
       "                                   journal  \n",
       "0  Computer Vision and Image Understanding  \n",
       "1  Computer Vision and Image Understanding  \n",
       "2  Computer Vision and Image Understanding  \n",
       "3  Computer Vision and Image Understanding  \n",
       "4  Computer Vision and Image Understanding  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the data\n",
    "data_df = pd.read_csv('../dataset/articles.csv')\n",
    "\n",
    "data_df.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journal</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Computer Vision and Image Understanding</th>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data &amp; Knowledge Engineering</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Journal of Visual Communication and Image Representation</th>\n",
       "      <td>433</td>\n",
       "      <td>433</td>\n",
       "      <td>433</td>\n",
       "      <td>433</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    entry  title  keywords  \\\n",
       "journal                                                                      \n",
       "Computer Vision and Image Understanding               249    249       249   \n",
       "Data & Knowledge Engineering                           99     99        99   \n",
       "Journal of Visual Communication and Image Repre...    433    433       433   \n",
       "\n",
       "                                                    abstract  year  \n",
       "journal                                                             \n",
       "Computer Vision and Image Understanding                  249   249  \n",
       "Data & Knowledge Engineering                              99    99  \n",
       "Journal of Visual Communication and Image Repre...       433   433  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Studying the data (there is an imbalance in the data)\n",
    "data_df.groupby(['journal']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the fouth column\n",
    "data_df['all_text'] = data_df['title'] + ' ' + data_df['keywords'] + ' ' + data_df['abstract'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the label columns\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "data_df['journal_label'] = le.fit_transform(data_df['journal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples is 781\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>journal</th>\n",
       "      <th>all_text</th>\n",
       "      <th>journal_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AZAZA20181</td>\n",
       "      <td>Context proposals for saliency detection</td>\n",
       "      <td>Computational saliency, Object segmentation, O...</td>\n",
       "      <td>One of the fundamental properties of a salient...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Computer Vision and Image Understanding</td>\n",
       "      <td>Context proposals for saliency detection Compu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BENSHABAT201812</td>\n",
       "      <td>Graph based over-segmentation methods for 3D p...</td>\n",
       "      <td>3D point cloud over-segmentation, 3D point clo...</td>\n",
       "      <td>Over-segmentation, or super-pixel generation, ...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Computer Vision and Image Understanding</td>\n",
       "      <td>Graph based over-segmentation methods for 3D p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YANG201843</td>\n",
       "      <td>Text effects transfer via distribution-aware t...</td>\n",
       "      <td>Text effects, Texture synthesis, Spatial distr...</td>\n",
       "      <td>In this paper, we explore the problem of fanta...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Computer Vision and Image Understanding</td>\n",
       "      <td>Text effects transfer via distribution-aware t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BARATH201870</td>\n",
       "      <td>Efficient energy-based topological outlier rej...</td>\n",
       "      <td>Stereo vision, Outlier filtering, Energy minim...</td>\n",
       "      <td>An approach is proposed for outlier rejection ...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Computer Vision and Image Understanding</td>\n",
       "      <td>Efficient energy-based topological outlier rej...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARRIGONI201895</td>\n",
       "      <td>Robust synchronization in SO(3) and SE(3) via ...</td>\n",
       "      <td>Absolute rotations, Global rotations, Structur...</td>\n",
       "      <td>This paper deals with the synchronization prob...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Computer Vision and Image Understanding</td>\n",
       "      <td>Robust synchronization in SO(3) and SE(3) via ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             entry                                              title  \\\n",
       "0       AZAZA20181           Context proposals for saliency detection   \n",
       "1  BENSHABAT201812  Graph based over-segmentation methods for 3D p...   \n",
       "2       YANG201843  Text effects transfer via distribution-aware t...   \n",
       "3     BARATH201870  Efficient energy-based topological outlier rej...   \n",
       "4   ARRIGONI201895  Robust synchronization in SO(3) and SE(3) via ...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  Computational saliency, Object segmentation, O...   \n",
       "1  3D point cloud over-segmentation, 3D point clo...   \n",
       "2  Text effects, Texture synthesis, Spatial distr...   \n",
       "3  Stereo vision, Outlier filtering, Energy minim...   \n",
       "4  Absolute rotations, Global rotations, Structur...   \n",
       "\n",
       "                                            abstract  year  \\\n",
       "0  One of the fundamental properties of a salient...  2018   \n",
       "1  Over-segmentation, or super-pixel generation, ...  2018   \n",
       "2  In this paper, we explore the problem of fanta...  2018   \n",
       "3  An approach is proposed for outlier rejection ...  2018   \n",
       "4  This paper deals with the synchronization prob...  2018   \n",
       "\n",
       "                                   journal  \\\n",
       "0  Computer Vision and Image Understanding   \n",
       "1  Computer Vision and Image Understanding   \n",
       "2  Computer Vision and Image Understanding   \n",
       "3  Computer Vision and Image Understanding   \n",
       "4  Computer Vision and Image Understanding   \n",
       "\n",
       "                                            all_text  journal_label  \n",
       "0  Context proposals for saliency detection Compu...              0  \n",
       "1  Graph based over-segmentation methods for 3D p...              0  \n",
       "2  Text effects transfer via distribution-aware t...              0  \n",
       "3  Efficient energy-based topological outlier rej...              0  \n",
       "4  Robust synchronization in SO(3) and SE(3) via ...              0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The number of samples is {}'.format(len(data_df)))\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe with information of interest\n",
    "data_to_proccess = data_df[['title', 'keywords', 'abstract', 'all_text', 'journal_label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>all_text</th>\n",
       "      <th>journal_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>context proposals saliency detection</td>\n",
       "      <td>computational saliency object segmentation obj...</td>\n",
       "      <td>one fundamental properties salient object regi...</td>\n",
       "      <td>context proposals saliency detection computati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>graph based oversegmentation methods 3d point ...</td>\n",
       "      <td>3d point cloud oversegmentation 3d point cloud...</td>\n",
       "      <td>oversegmentation superpixel generation common ...</td>\n",
       "      <td>graph based oversegmentation methods 3d point ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text effects transfer via distributionaware te...</td>\n",
       "      <td>text effects texture synthesis spatial distrib...</td>\n",
       "      <td>paper explore problem fantastic specialeffects...</td>\n",
       "      <td>text effects transfer via distributionaware te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>efficient energybased topological outlier reje...</td>\n",
       "      <td>stereo vision outlier filtering energy minimiz...</td>\n",
       "      <td>approach proposed outlier rejection set 2d poi...</td>\n",
       "      <td>efficient energybased topological outlier reje...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robust synchronization so3 se3 via lowrank spa...</td>\n",
       "      <td>absolute rotations global rotations structuref...</td>\n",
       "      <td>paper deals synchronization problem arises mul...</td>\n",
       "      <td>robust synchronization so3 se3 via lowrank spa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0               context proposals saliency detection   \n",
       "1  graph based oversegmentation methods 3d point ...   \n",
       "2  text effects transfer via distributionaware te...   \n",
       "3  efficient energybased topological outlier reje...   \n",
       "4  robust synchronization so3 se3 via lowrank spa...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  computational saliency object segmentation obj...   \n",
       "1  3d point cloud oversegmentation 3d point cloud...   \n",
       "2  text effects texture synthesis spatial distrib...   \n",
       "3  stereo vision outlier filtering energy minimiz...   \n",
       "4  absolute rotations global rotations structuref...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  one fundamental properties salient object regi...   \n",
       "1  oversegmentation superpixel generation common ...   \n",
       "2  paper explore problem fantastic specialeffects...   \n",
       "3  approach proposed outlier rejection set 2d poi...   \n",
       "4  paper deals synchronization problem arises mul...   \n",
       "\n",
       "                                            all_text  journal_label  \n",
       "0  context proposals saliency detection computati...              0  \n",
       "1  graph based oversegmentation methods 3d point ...              0  \n",
       "2  text effects transfer via distributionaware te...              0  \n",
       "3  efficient energybased topological outlier reje...              0  \n",
       "4  robust synchronization so3 se3 via lowrank spa...              0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing punctuation signs, lowercase, stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "column_names = ['title', 'keywords', 'abstract', 'all_text']\n",
    "\n",
    "for column in column_names:\n",
    "    data_to_proccess[column] = data_to_proccess[column].str.replace(r'[^\\w\\s]+', '')\n",
    "    data_to_proccess[column] = data_to_proccess[column].str.lower()\n",
    "    data_to_proccess[column] = data_to_proccess[column].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    \n",
    "data_to_proccess.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>all_text</th>\n",
       "      <th>journal_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>context propos salienc detect</td>\n",
       "      <td>comput salienc object segment object propos</td>\n",
       "      <td>one fundament properti salient object region c...</td>\n",
       "      <td>context propos salienc detect comput salienc o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>graph base oversegment method 3d point cloud</td>\n",
       "      <td>3d point cloud oversegment 3d point cloud segm...</td>\n",
       "      <td>oversegment superpixel gener common preliminar...</td>\n",
       "      <td>graph base oversegment method 3d point cloud 3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text effect transfer via distributionawar text...</td>\n",
       "      <td>text effect textur synthesi spatial distribut ...</td>\n",
       "      <td>paper explor problem fantast specialeffect syn...</td>\n",
       "      <td>text effect transfer via distributionawar text...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>effici energybas topolog outlier reject</td>\n",
       "      <td>stereo vision outlier filter energi minim poin...</td>\n",
       "      <td>approach propos outlier reject set 2d point co...</td>\n",
       "      <td>effici energybas topolog outlier reject stereo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robust synchron so3 se3 via lowrank spars matr...</td>\n",
       "      <td>absolut rotat global rotat structurefrommot gl...</td>\n",
       "      <td>paper deal synchron problem aris multipl 3d po...</td>\n",
       "      <td>robust synchron so3 se3 via lowrank spars matr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                      context propos salienc detect   \n",
       "1       graph base oversegment method 3d point cloud   \n",
       "2  text effect transfer via distributionawar text...   \n",
       "3            effici energybas topolog outlier reject   \n",
       "4  robust synchron so3 se3 via lowrank spars matr...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0        comput salienc object segment object propos   \n",
       "1  3d point cloud oversegment 3d point cloud segm...   \n",
       "2  text effect textur synthesi spatial distribut ...   \n",
       "3  stereo vision outlier filter energi minim poin...   \n",
       "4  absolut rotat global rotat structurefrommot gl...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  one fundament properti salient object region c...   \n",
       "1  oversegment superpixel gener common preliminar...   \n",
       "2  paper explor problem fantast specialeffect syn...   \n",
       "3  approach propos outlier reject set 2d point co...   \n",
       "4  paper deal synchron problem aris multipl 3d po...   \n",
       "\n",
       "                                            all_text  journal_label  \n",
       "0  context propos salienc detect comput salienc o...              0  \n",
       "1  graph base oversegment method 3d point cloud 3...              0  \n",
       "2  text effect transfer via distributionawar text...              0  \n",
       "3  effici energybas topolog outlier reject stereo...              0  \n",
       "4  robust synchron so3 se3 via lowrank spars matr...              0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Porter\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for column in column_names:\n",
    "    data_to_proccess[column] = data_to_proccess[column].apply(lambda x: ' '.join([porter.stem(word) for word in x.split()]))\n",
    "\n",
    "data_to_proccess.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature extraction (BoW, TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF definition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = 'english',\n",
    "        sublinear_tf = True,\n",
    "        strip_accents = 'ascii',\n",
    "        analyzer = 'word',\n",
    "        token_pattern = r'\\w{2,}', \n",
    "        ngram_range = (1, 3),\n",
    "        max_features = 800,\n",
    "        max_df = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_list = []\n",
    "\n",
    "for column in column_names:\n",
    "    tf_idf_list.append(tfidf.fit_transform(data_to_proccess[column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to visualize tfidf output\n",
    "def visualize_words_and_frequency_tfidf(tfidf_list, method):\n",
    "\n",
    "    weights = np.asarray(tfidf_list.mean(axis = 0)).ravel().tolist()\n",
    "    weights_df = pd.DataFrame({'Word': method.get_feature_names(), 'Weight': weights})\n",
    "    weights_df = weights_df.sort_values(by = 'Weight', ascending = False).head()\n",
    "    \n",
    "    print(weights_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Word    Weight\n",
      "331    imag  0.055628\n",
      "424  method  0.047048\n",
      "759     use  0.040084\n",
      "434   model  0.037070\n",
      "270  featur  0.035079\n"
     ]
    }
   ],
   "source": [
    "visualize_words_and_frequency_tfidf(tf_idf_list[3], tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2d</th>\n",
       "      <th>3d</th>\n",
       "      <th>3d model</th>\n",
       "      <th>3d reconstruct</th>\n",
       "      <th>3dhevc</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>accord</th>\n",
       "      <th>account</th>\n",
       "      <th>accur</th>\n",
       "      <th>...</th>\n",
       "      <th>way</th>\n",
       "      <th>web</th>\n",
       "      <th>weight</th>\n",
       "      <th>wide</th>\n",
       "      <th>wide use</th>\n",
       "      <th>window</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>year</th>\n",
       "      <th>yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.105624</td>\n",
       "      <td>0.238512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094357</td>\n",
       "      <td>0.101706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.116179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.152822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         2d        3d  3d model  3d reconstruct  3dhevc  abil  abl    accord  \\\n",
       "0  0.000000  0.000000       0.0             0.0     0.0   0.0  0.0  0.000000   \n",
       "1  0.105624  0.238512       0.0             0.0     0.0   0.0  0.0  0.094357   \n",
       "2  0.000000  0.000000       0.0             0.0     0.0   0.0  0.0  0.000000   \n",
       "3  0.152822  0.000000       0.0             0.0     0.0   0.0  0.0  0.000000   \n",
       "4  0.000000  0.098776       0.0             0.0     0.0   0.0  0.0  0.000000   \n",
       "\n",
       "    account     accur  ...       way  web    weight  wide  wide use  window  \\\n",
       "0  0.000000  0.000000  ...  0.100921  0.0  0.000000   0.0       0.0     0.0   \n",
       "1  0.101706  0.000000  ...  0.000000  0.0  0.091337   0.0       0.0     0.0   \n",
       "2  0.000000  0.000000  ...  0.000000  0.0  0.000000   0.0       0.0     0.0   \n",
       "3  0.000000  0.117648  ...  0.000000  0.0  0.000000   0.0       0.0     0.0   \n",
       "4  0.000000  0.000000  ...  0.000000  0.0  0.000000   0.0       0.0     0.0   \n",
       "\n",
       "       word  work  year  yield  \n",
       "0  0.000000   0.0   0.0    0.0  \n",
       "1  0.000000   0.0   0.0    0.0  \n",
       "2  0.116179   0.0   0.0    0.0  \n",
       "3  0.000000   0.0   0.0    0.0  \n",
       "4  0.000000   0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 800 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tf_idf_list[3].toarray(), columns=tfidf.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words definition\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english',\n",
    "        strip_accents = 'ascii',\n",
    "        analyzer = 'word',\n",
    "        token_pattern = r'\\w{3,}',\n",
    "        ngram_range = (1, 3),\n",
    "        max_features = 800,\n",
    "        max_df = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_list = []\n",
    "\n",
    "for column in column_names:\n",
    "    cv_list.append(cv.fit_transform(data_to_proccess[column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Word    Weight\n",
      "327    imag  2.591549\n",
      "421  method  1.791293\n",
      "758     use  1.377721\n",
      "431   model  1.153649\n",
      "265  featur  1.021767\n"
     ]
    }
   ],
   "source": [
    "visualize_words_and_frequency_tfidf(cv_list[3], cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3dhevc</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>accord</th>\n",
       "      <th>account</th>\n",
       "      <th>accur</th>\n",
       "      <th>accuraci</th>\n",
       "      <th>achiev</th>\n",
       "      <th>action</th>\n",
       "      <th>action recognit</th>\n",
       "      <th>...</th>\n",
       "      <th>web</th>\n",
       "      <th>weight</th>\n",
       "      <th>wide</th>\n",
       "      <th>wide use</th>\n",
       "      <th>window</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>work propos</th>\n",
       "      <th>year</th>\n",
       "      <th>yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   3dhevc  abil  abl  accord  account  accur  accuraci  achiev  action  \\\n",
       "0       0     0    0       0        0      0         0       0       0   \n",
       "1       0     0    0       1        1      0         0       0       0   \n",
       "2       0     0    0       0        0      0         0       0       0   \n",
       "3       0     0    0       0        0      1         0       1       0   \n",
       "4       0     0    0       0        0      0         0       0       0   \n",
       "\n",
       "   action recognit  ...  web  weight  wide  wide use  window  word  work  \\\n",
       "0                0  ...    0       0     0         0       0     0     0   \n",
       "1                0  ...    0       1     0         0       0     0     0   \n",
       "2                0  ...    0       0     0         0       0     1     0   \n",
       "3                0  ...    0       0     0         0       0     0     0   \n",
       "4                0  ...    0       0     0         0       0     0     0   \n",
       "\n",
       "   work propos  year  yield  \n",
       "0            0     0      0  \n",
       "1            0     0      0  \n",
       "2            0     0      0  \n",
       "3            0     0      0  \n",
       "4            0     0      0  \n",
       "\n",
       "[5 rows x 800 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv_list[3].toarray(), columns=cv.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dimensionality Reduction (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de atributos calculados con TF-IDF es: 800\n",
      "\n",
      "---Reducción de dimensionalidad con LSA---\n",
      "Ha llevado 7.405 segundos\n",
      "Cantidad de información contenida en LSA (varianza): 97%\n"
     ]
    }
   ],
   "source": [
    "#print(\"El número de atributos calculados con TF-IDF es: %d\" % docTrans0.get_shape()[1])\n",
    "\n",
    "print(\"\\n---Dimensionality reduction with LSA---\")\n",
    "\n",
    "%%time\n",
    "\n",
    "svd = TruncatedSVD(n_components = 500, n_iter = 100, random_state = 42)\n",
    "normalizer = Normalizer(copy = False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "for name in column_names:\n",
    "    \n",
    "\n",
    "tfidf_matrix_lsa0 = lsa.fit_transform(tfidf_matrix_list[0][0])\n",
    "tfidf_matrix_lsa1 = lsa.fit_transform(tfidf_matrix_list[0][1])\n",
    "tfidf_matrix_lsa2 = lsa.fit_transform(tfidf_matrix_list[0][2])\n",
    "\n",
    "tfidf_matrix_lsa = [[tfidf_matrix_lsa0, tfidf_matrix_lsa1, tfidf_matrix_lsa2]]\n",
    "\n",
    "print(\"Ha llevado %.3f segundos\" % (time.time() - t0))\n",
    "\n",
    "variance_lsa = svd.explained_variance_ratio_.sum()\n",
    "\n",
    "print(\"Cantidad de información contenida en LSA (varianza): {}%\".format(int(variance_lsa * 100)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
